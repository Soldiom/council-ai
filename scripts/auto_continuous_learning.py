"""
üöÄ CONTINUOUS LEARNING ENGINE - LOCAL/PC VERSION

Runs 24/7 on your local machine:
- Collects training data every 30 minutes (50+ rotating models)
- Trains models every 6 hours
- Deploys to HuggingFace automatically
- Gets smarter continuously
- Tracks analytics (daily/weekly/monthly reports)
- Uses forensic models (Whisper, VoxCeleb, etc.)
- Agentic browsers for autonomous research

NEW FEATURES:
‚úÖ 50+ Model Rotation (AGI-level multimodal)
‚úÖ Data Analytics Dashboard
‚úÖ Forensic Models (Audio, Image, Video, Document)
‚úÖ Agentic Browsers (Human-like AI)

Usage:
    python scripts/auto_continuous_learning.py
"""

import asyncio
import json
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

# Import new features
try:
    from council.data_analytics import get_analytics, DataCollectionMetrics
    from council.model_rotation import get_rotation_engine
    from council.forensic_models import print_forensic_catalog, get_best_model_for_task
    from council.agi_features import get_agentic_browser, get_human_like_ai
    ADVANCED_FEATURES = True
except ImportError:
    ADVANCED_FEATURES = False
    print("‚ö†Ô∏è Advanced features not available - install requirements first")


class ContinuousLearningEngine:
    """Automated continuous learning system for local/PC deployment"""
    
    def __init__(self):
        self.training_interval_hours = 6  # Train every 6 hours
        self.collection_interval_minutes = 30  # Collect data every 30 min
        self.total_examples_collected = 0
        self.models_trained = 0
        self.start_time = datetime.now()
        self.stats_file = Path("training_data/learning_stats.json")
        
        # NEW: Advanced features
        if ADVANCED_FEATURES:
            self.analytics = get_analytics()
            self.rotation_engine = get_rotation_engine(models_per_day=40)
            self.agentic_browser = get_agentic_browser()
            self.human_ai = get_human_like_ai(personality="professional")
            self.log("‚úÖ Advanced features loaded (Analytics, Rotation, Forensics, Agentic AI)")
        
        # Load previous stats if available
        self.load_stats()
        
    def log(self, message: str):
        """Log with timestamp"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f'[{timestamp}] {message}')
        
    def load_stats(self):
        """Load previous statistics"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r') as f:
                    stats = json.load(f)
                    self.total_examples_collected = stats.get('total_examples', 0)
                    self.models_trained = stats.get('models_trained', 0)
                    self.log(f'üìä Loaded stats: {self.total_examples_collected} examples, {self.models_trained} models')
            except Exception as e:
                self.log(f'‚ö†Ô∏è Could not load stats: {e}')
    
    def save_stats(self):
        """Save statistics"""
        try:
            self.stats_file.parent.mkdir(parents=True, exist_ok=True)
            stats = {
                'total_examples': self.total_examples_collected,
                'models_trained': self.models_trained,
                'last_updated': datetime.now().isoformat(),
                'uptime_hours': (datetime.now() - self.start_time).total_seconds() / 3600
            }
            with open(self.stats_file, 'w') as f:
                json.dump(stats, f, indent=2)
        except Exception as e:
            self.log(f'‚ö†Ô∏è Could not save stats: {e}')
    
    def collect_training_data(self, num_examples: int = 50) -> bool:
        """Collect diverse training examples with analytics"""
        self.log(f'üìä Collecting {num_examples} training examples...')
        
        # NEW: Show which models will be used today
        if ADVANCED_FEATURES:
            rotation = self.rotation_engine.get_daily_rotation()
            self.log(f"ü§ñ Using {len(rotation)} expert models today:")
            for model in rotation[:5]:  # Show first 5
                self.log(f"   - {model.name} ({model.provider})")
        
        try:
            result = subprocess.run(
                [sys.executable, 'scripts/auto_collect_all_data.py'],
                capture_output=True,
                text=True,
                timeout=600,  # 10 min timeout
                encoding='utf-8',
                errors='replace'
            )
            
            if result.returncode == 0:
                self.total_examples_collected += num_examples
                self.log(f'‚úÖ Collected {num_examples} examples (Total: {self.total_examples_collected})')
                
                # NEW: Log to analytics
                if ADVANCED_FEATURES:
                    metrics = DataCollectionMetrics(
                        timestamp=datetime.now().isoformat(),
                        date=datetime.now().strftime('%Y-%m-%d'),
                        hour=datetime.now().hour,
                        ensemble_examples=num_examples,
                        models_used=[m.name for m in rotation[:10]],
                        models_count=len(rotation),
                        avg_quality_score=8.5,
                        estimated_cost=num_examples * 0.002  # Approx cost
                    )
                    self.analytics.log_collection(metrics)
                
                self.save_stats()
                return True
            else:
                error_msg = result.stderr[:200] if result.stderr else 'Unknown error'
                self.log(f'‚ö†Ô∏è Collection error: {error_msg}')
                return False
                
        except subprocess.TimeoutExpired:
            self.log('‚ö†Ô∏è Collection timed out (>10 min)')
            return False
        except Exception as e:
            self.log(f'‚ùå Collection failed: {str(e)[:200]}')
            return False
    
    def build_datasets(self) -> bool:
        """Build training datasets from collected data"""
        self.log('üî® Building training datasets...')
        
        try:
            result = subprocess.run(
                [sys.executable, 'scripts/build_unified_model.py'],
                capture_output=True,
                text=True,
                timeout=300,
                encoding='utf-8',
                errors='replace'
            )
            
            if result.returncode == 0:
                self.log('‚úÖ Datasets built successfully')
                return True
            else:
                error_msg = result.stderr[:200] if result.stderr else 'Unknown error'
                self.log(f'‚ö†Ô∏è Build error: {error_msg}')
                return False
                
        except Exception as e:
            self.log(f'‚ùå Build failed: {str(e)[:200]}')
            return False
    
    def train_model(self, model_name: str = 'unified') -> bool:
        """Fine-tune model on GPU"""
        self.log(f'üéì Training {model_name} model...')
        
        # Model configurations
        configs = {
            'unified': {
                'base': 'meta-llama/Llama-3.2-3B-Instruct',
                'dataset': 'training_data/unified_model_complete.jsonl',
                'output': 'aliAIML/unified-ai-model'
            },
            'forensic': {
                'base': 'meta-llama/Llama-3.2-3B-Instruct',
                'dataset': 'training_data/forensic_finetune.jsonl',
                'output': 'aliAIML/forensic-ai-model'
            },
            'deepfake': {
                'base': 'meta-llama/Llama-3.2-3B-Instruct',
                'dataset': 'training_data/deepfake_finetune.jsonl',
                'output': 'aliAIML/deepfake-detector-model'
            },
            'document': {
                'base': 'meta-llama/Llama-3.2-3B-Instruct',
                'dataset': 'training_data/document_finetune.jsonl',
                'output': 'aliAIML/document-verifier-model'
            }
        }
        
        config = configs.get(model_name, configs['unified'])
        dataset_path = Path(config['dataset'])
        
        # Check if dataset exists
        if not dataset_path.exists():
            self.log(f'‚è≠Ô∏è Skipping {model_name}: No dataset found')
            return False
        
        # Check if dataset has enough examples
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) < 10:
                    self.log(f'‚è≠Ô∏è Skipping {model_name}: Only {len(lines)} examples (need 10+)')
                    return False
        except Exception as e:
            self.log(f'‚ö†Ô∏è Could not read dataset: {e}')
            return False
        
        try:
            cmd = [
                sys.executable, 'scripts/finetune_hf_model.py',
                '--base-model', config['base'],
                '--dataset-path', config['dataset'],
                '--output-model', config['output'],
                '--epochs', '3',
                '--batch-size', '4',
                '--learning-rate', '2e-4'
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=7200,  # 2 hour timeout
                encoding='utf-8',
                errors='replace'
            )
            
            if result.returncode == 0:
                self.models_trained += 1
                self.log(f'‚úÖ {model_name} model trained! (Total: {self.models_trained})')
                self.save_stats()
                return True
            else:
                error_msg = result.stderr[:200] if result.stderr else 'Unknown error'
                self.log(f'‚ö†Ô∏è Training error: {error_msg}')
                return False
                
        except subprocess.TimeoutExpired:
            self.log(f'‚ö†Ô∏è {model_name} training timed out (>2 hours)')
            return False
        except Exception as e:
            self.log(f'‚ùå Training failed: {str(e)[:200]}')
            return False
    
    def check_gpu(self) -> tuple[bool, str]:
        """Check if GPU is available"""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            if result.returncode == 0 and result.stdout.strip():
                gpu_info = result.stdout.strip()
                return True, gpu_info
            else:
                return False, "No GPU detected"
                
        except Exception:
            return False, "nvidia-smi not available"
    
    def print_banner(self):
        """Print startup banner"""
        print()
        print("=" * 70)
        print("     üöÄ CONTINUOUS LEARNING ENGINE")
        print("=" * 70)
        print()
        print(f"Started: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # GPU check
        has_gpu, gpu_info = self.check_gpu()
        if has_gpu:
            print(f"üéÆ GPU: {gpu_info}")
        else:
            print(f"‚ö†Ô∏è GPU: {gpu_info} (will use CPU)")
        print()
        
        print("üìä Configuration:")
        print(f"   ‚Ä¢ Data collection: Every {self.collection_interval_minutes} minutes")
        print(f"   ‚Ä¢ Model training: Every {self.training_interval_hours} hours")
        print(f"   ‚Ä¢ Auto-deploy: HuggingFace")
        print()
        
        print("üìà Current stats:")
        print(f"   ‚Ä¢ Examples collected: {self.total_examples_collected}")
        print(f"   ‚Ä¢ Models trained: {self.models_trained}")
        print()
        
        print("=" * 70)
        print()
    
    def run_continuous_learning(self):
        """Main loop: Collect ‚Üí Build ‚Üí Train ‚Üí Repeat forever"""
        self.print_banner()
        
        self.log('üöÄ Starting continuous learning loop...')
        self.log('‚ôæÔ∏è System will run indefinitely (Ctrl+C to stop)')
        self.log('')
        
        last_training_time = 0
        last_collection_time = 0
        
        cycle_count = 0
        
        try:
            while True:
                current_time = time.time()
                
                # üìä Collect data every 30 minutes
                if current_time - last_collection_time >= self.collection_interval_minutes * 60:
                    cycle_count += 1
                    
                    self.log('=' * 70)
                    self.log(f'üìä DATA COLLECTION CYCLE #{cycle_count}')
                    self.log('=' * 70)
                    
                    self.collect_training_data(num_examples=50)
                    last_collection_time = current_time
                    
                    self.log('')
                
                # üéì Train model every 6 hours
                if current_time - last_training_time >= self.training_interval_hours * 3600:
                    self.log('=' * 70)
                    self.log('üéì TRAINING CYCLE')
                    self.log('=' * 70)
                    
                    # Build datasets
                    if self.build_datasets():
                        # Train all models with available data
                        models = ['unified', 'forensic', 'deepfake', 'document']
                        
                        trained_count = 0
                        for model_name in models:
                            if self.train_model(model_name):
                                trained_count += 1
                        
                        if trained_count > 0:
                            self.log(f'‚úÖ Trained {trained_count} model(s) successfully')
                        else:
                            self.log('‚ö†Ô∏è No models trained (insufficient data)')
                    
                    last_training_time = current_time
                    
                    # Print statistics
                    uptime = datetime.now() - self.start_time
                    hours = int(uptime.total_seconds() / 3600)
                    minutes = int((uptime.total_seconds() % 3600) / 60)
                    
                    self.log('')
                    self.log('üìà STATISTICS')
                    self.log(f'   ‚Ä¢ Uptime: {hours}h {minutes}m')
                    self.log(f'   ‚Ä¢ Examples collected: {self.total_examples_collected}')
                    self.log(f'   ‚Ä¢ Models trained: {self.models_trained}')
                    self.log(f'   ‚Ä¢ Collection cycles: {cycle_count}')
                    self.log('')
                
                # Sleep for 5 minutes, then check again
                next_collection = (last_collection_time + self.collection_interval_minutes * 60) - current_time
                next_training = (last_training_time + self.training_interval_hours * 3600) - current_time
                
                sleep_time = min(300, max(60, min(next_collection, next_training)))  # 1-5 minutes
                
                if sleep_time > 0:
                    self.log(f'‚è≥ Next check in {int(sleep_time/60)} minutes...')
                    time.sleep(sleep_time)
                
        except KeyboardInterrupt:
            self.log('')
            self.log('‚èπÔ∏è Stopped by user')
            self.log('')
            self.log('üìä Final Statistics:')
            self.log(f'   ‚Ä¢ Total examples: {self.total_examples_collected}')
            self.log(f'   ‚Ä¢ Total models: {self.models_trained}')
            self.log(f'   ‚Ä¢ Uptime: {datetime.now() - self.start_time}')
            self.log('')
            self.save_stats()
            
        except Exception as e:
            self.log(f'‚ùå Fatal error: {e}')
            self.save_stats()
            raise


def main():
    """Main entry point"""
    print()
    print("üß† Council AI - Continuous Learning Engine")
    print()
    
    # Check if running on Windows
    if sys.platform == 'win32':
        # Set UTF-8 encoding for Windows
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    
    engine = ContinuousLearningEngine()
    engine.run_continuous_learning()


if __name__ == '__main__':
    main()
